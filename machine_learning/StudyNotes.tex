\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{array,enumitem,layouts,calc}
\usepackage[left=2.2cm,right=2.2cm,top=2.0cm,bottom=2.0cm]{geometry}
\setlist{nosep,leftmargin=2em,rightmargin=0em}
%\usepackage{mathptmx}
\usepackage[T1]{fontenc}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\newcommand\leftColumn{0.75\textwidth-2\tabcolsep}
\newcommand\rightColumn{0.25\textwidth-2\tabcolsep}
\newcommand\singleColumn{\textwidth-2\tabcolsep}
\newcommand{\ket}[1]{ $\left| #1 \right>$ }
\setlength{\tabcolsep}{0pt}

\begin{document}
\noindent{\large \textbf{30-Aug-2020}: Welcome Video, and What is Machine Learning}
\begin{itemize}
  \item Machine learning: algorithms; supervised, unsupervised, reinforcement, recommender. In this course, also will learn best practices.
\end{itemize}
\hfill \\
{\large \textbf{31-Aug-2020}: Supervised Learning, and Unsupervised Learning}
\begin{itemize}
  \item Supervised learning: right answers are given
  \item Regression: predicts continuous variable output; Classification: predicts discrete values
  \item Classification can have $1,\dots,N,\dots,\infty$ attributes. E.g. benignness/malignancy based on age, or age and tumor size, etc.
  \item Unsupervised learning a.k.a. clustering: Right answers aren't given. For example, news that links to different sources for the same topic.
  \item Cocktail party algorithm: separates two voices in a conversation, with two microphone recordings. Singular value decomposition is key to this algorithm.
  \item When learning machine learning, use Octave
\end{itemize}
\hfill \\
{\large \textbf{1-Sep-2020}: Model Representation, and Cost Function}
\begin{itemize}
  \item Training set notation: $m$ is number of training examples, $x$ are input examples, and $y$ are the output variables. Together, $(x,y)$ form a training example. Also denoted $(x^{(i)},y^{(i)})$.
  \item In a linear regression, $h_{\theta}(x) = \theta_0 + \theta_1 x \equiv h(x)$.
  \item Cost function is 
    \begin{equation*}
      J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
    \end{equation*}
  \item Want to minimize $J$ w.r.t. $\theta_0$ and $\theta_1$.
\end{itemize}
{\large \textbf{4-Sep-2020}: Cost Function, Intuition I\&II; Gradient Descent}
\begin{itemize}
  \item Intuition I; Let $\theta_0=0$, then $\min_{\theta_1}J(\theta_1)$ is what we want
  \item Ex: $h_{\theta}(x)=\theta_1x$ and let $(x,y) = \{ (1,1), (2,2), (3,3) \}$.
    \begin{align*}
      J(\theta_1) &= \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 \\
      &\rightarrow \text{ If $\theta_1=0$, $h_{\theta}(x) \equiv 0$} \\
      J(0) &= \frac{1}{2 \times 3} \left( 1 + 4+9\right) \\ 
      &= \frac{14}{6}
    \end{align*}
  \item $J(\theta_1)$ is parabolic
  \item We want $\min_{\theta}J(\theta)$; here, $\theta_1=1$ satisfies this criterion
  \item Intuition II; Let $\theta_0, \theta_1$ be free in $J(\theta_0, \theta_1)$ and $h_{\theta}(x)$.
  \item $J(\theta_0, theta_1)$ is a parabloid
  \item Gradient Descent; Use gradient descent to find $(\theta_0, \theta_1)$ that minimizes $J(\theta_0, \theta_1)$.
  \item Differing starting guesses can give different local minima.
  \item Gradient descent algorithm:
    \begin{equation*}
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad \text{for $j=1,2$}
    \end{equation*}
  \item Simultaneously update $\theta_0, \theta_1$, $\alpha$ is called the learning rate.
  \item Ex: $\theta_0=1, \theta_2=2$ and $\theta_j := \theta_j + \sqrt{\theta_0 \theta_1}$.
    \begin{align*}
      \theta_0 &:= \theta_0 + \sqrt{\theta_0 \theta_1} \\
      &= 1 + \sqrt{1 \times 2} \\
      &= 1 + \sqrt{2} \\
      \theta_1 &= \theta_2 + \sqrt{ \theta_0 \theta_1 } \\
      &= 2 + \sqrt{1 \times 2} \quad \text{note here that we used the old value of $\theta_0$} \\
      &= 2 + \sqrt{2}
    \end{align*}
\end{itemize}
\end{document}
