\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{array,enumitem,layouts,calc}
\usepackage[left=2.2cm,right=2.2cm,top=2.0cm,bottom=2.0cm]{geometry}
\setlist{nosep,leftmargin=2em,rightmargin=0em}
%\usepackage{mathptmx}
\usepackage[T1]{fontenc}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\newcommand\leftColumn{0.75\textwidth-2\tabcolsep}
\newcommand\rightColumn{0.25\textwidth-2\tabcolsep}
\newcommand\singleColumn{\textwidth-2\tabcolsep}
\newcommand{\ket}[1]{ $\left| #1 \right>$ }
\setlength{\tabcolsep}{0pt}

\begin{document}
\noindent{\large \textbf{30-Aug-2020}: Welcome Video, and What is Machine Learning}
\begin{itemize}
  \item Machine learning: algorithms; supervised, unsupervised, reinforcement, recommender. In this course, also will learn best practices.
\end{itemize}
\hfill \\
{\large \textbf{31-Aug-2020}: Supervised Learning, and Unsupervised Learning}
\begin{itemize}
  \item Supervised learning: right answers are given
  \item Regression: predicts continuous variable output; Classification: predicts discrete values
  \item Classification can have $1,\dots,N,\dots,\infty$ attributes. E.g. benignness/malignancy based on age, or age and tumor size, etc.
  \item Unsupervised learning a.k.a. clustering: Right answers aren't given. For example, news that links to different sources for the same topic.
  \item Cocktail party algorithm: separates two voices in a conversation, with two microphone recordings. Singular value decomposition is key to this algorithm.
  \item When learning machine learning, use Octave
\end{itemize}
\hfill \\
{\large \textbf{1-Sep-2020}: Model Representation, and Cost Function}
\begin{itemize}
  \item Training set notation: $m$ is number of training examples, $x$ are input examples, and $y$ are the output variables. Together, $(x,y)$ form a training example. Also denoted $(x^{(i)},y^{(i)})$.
  \item In a linear regression, $h_{\theta}(x) = \theta_0 + \theta_1 x \equiv h(x)$.
  \item Cost function is 
    \begin{equation*}
      J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
    \end{equation*}
  \item Want to minimize $J$ w.r.t. $\theta_0$ and $\theta_1$.
\end{itemize}
{\large \textbf{4-Sep-2020}: Cost Function, Intuition I\&II; Gradient Descent}
\begin{itemize}
  \item Intuition I; Let $\theta_0=0$, then $\min_{\theta_1}J(\theta_1)$ is what we want
  \item Ex: $h_{\theta}(x)=\theta_1x$ and let $(x,y) = \{ (1,1), (2,2), (3,3) \}$.
    \begin{align*}
      J(\theta_1) &= \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 \\
      &\rightarrow \text{ If $\theta_1=0$, $h_{\theta}(x) \equiv 0$} \\
      J(0) &= \frac{1}{2 \times 3} \left( 1 + 4+9\right) \\ 
      &= \frac{14}{6}
    \end{align*}
  \item $J(\theta_1)$ is parabolic
  \item We want $\min_{\theta}J(\theta)$; here, $\theta_1=1$ satisfies this criterion
  \item Intuition II; Let $\theta_0, \theta_1$ be free in $J(\theta_0, \theta_1)$ and $h_{\theta}(x)$.
  \item $J(\theta_0, \theta_1)$ is a parabloid
  \item Gradient Descent; Use gradient descent to find $(\theta_0, \theta_1)$ that minimizes $J(\theta_0, \theta_1)$.
  \item Differing starting guesses can give different local minima.
  \item Gradient descent algorithm:
    \begin{equation*}
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad \text{for $j=1,2$}
    \end{equation*}
  \item Simultaneously update $\theta_0, \theta_1$, $\alpha$ is called the learning rate.
  \item Ex: $\theta_0=1, \theta_2=2$ and $\theta_j := \theta_j + \sqrt{\theta_0 \theta_1}$.
    \begin{align*}
      \theta_0 &:= \theta_0 + \sqrt{\theta_0 \theta_1} \\
      &= 1 + \sqrt{1 \times 2} \\
      &= 1 + \sqrt{2} \\
      \theta_1 &= \theta_2 + \sqrt{ \theta_0 \theta_1 } \\
      &= 2 + \sqrt{1 \times 2} \quad \text{note here that we used the old value of $\theta_0$} \\
      &= 2 + \sqrt{2}
    \end{align*}
\end{itemize}
{\large \textbf{5-Sep-2020}: Gradient Descent Intuition, Gradient Descent for Linear Regression}
\begin{itemize}
  \item Gradient Descent Intuition: For simplicity, assume $\theta_0=0$
  \item One variable: $\theta_1 := \theta_1 - \alpha \frac{d}{d \theta_1}J(\theta_1)$; Newton-Raphson
  \item If $\alpha$ is too small, convergence may be very slow. If too large, it may miss the minimum.
  \item If $\theta_1$ is already at a local minimum, g.d. leaves $\theta_1$ unchanged since the derivative is zero.
  \item Gradient Descent for Linear Regression: We need derivatives
    \begin{align*}
      \frac{ \partial }{\partial \theta_0} J(\theta_0, \theta_1) &= \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)}\right) \\
      \frac{ \partial }{\partial \theta_1} J(\theta_0, \theta_1) &= \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)}\right) \times x^{(i)}
    \end{align*}
  \item So, gradient descent finds the new $\theta$ variables as
    \begin{align*}
      \theta_0 &:= \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1x^{(i)} - y^{(i)} \right)\\
      \theta_1 &:= \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1x^{(i)} - y^{(i)} \right) \times x^{(i)}
    \end{align*}
  \item This is called ``batch gradient descent''; batch implies looking at all the training examples. This is represented by the $\sum_{i=1}^m$.
  \item Quiz Linear Regression with One Variable: 2) $m= \Delta y/ \Delta x = (1-0.5)/(2-1) = 0.5 \implies y=0.5x + b$; y-intercept is clearly zero since (0,0) is a data point.
  \item 3) $h_{\theta}(x)$; $\theta_0=-1$, $\theta_1=2$; $h_{\theta}(6) = -1 + 2 \times 6 = 11$
\end{itemize}
\hfill \\
{\large \textbf{9-Sep-2020}: Linear Algebra Review}
\begin{itemize}
  \item Matrices and Vectors: Nothing new; in this course, index from 1.
  \item Addition and Scalar Multiplication: Nothing new
  \item Matrix Vector Multiplication: Nothing new;
  \item Ex: Let house sizes be $\{2104, 1416, 1534, 852.\}$. Let the hypothesis be $h_{\theta}(x) = -40 + 0.25x$.
    \begin{equation*}
      \begin{bmatrix}
        1 & 2104 \\
        1 & 1416 \\
        1 & 1534 \\
        2 & 852 
      \end{bmatrix}
      \begin{bmatrix}
        -40 \\ 
        0.25
      \end{bmatrix} 
      =
      \begin{bmatrix}
        -40 \times 1 + 0.25 \times 2104 \\
        -40 \times 1 + 0.25 \times 1416 \\
        -40 \times 1 + 0.25 \times 1534 \\
        -40 \times 1 + 0.25 \times 852
      \end{bmatrix}
      =
      \begin{bmatrix}
        h_{\theta}(2104)\\
        h_{\theta}(1416)\\
        h_{\theta}(1534)\\
        h_{\theta}(852)
      \end{bmatrix}
    \end{equation*}
    This essentially says data matrix $\times$ parameters = prediction
  \item Best to do this with built-in linear algebra function in Octave/Python. You can do it manually in a for-loop, but it'll be really slow.
  \item Matrix Multiplication: Take the same example. Now we have three hypotheses:
    \begin{align*}
      h_{\theta}(x) &= -40 + 0.25x \\
      h_{\theta}(x) &= 200 + 0.1x \\
      h_{\theta}(x) &= -150 + 0.4x 
    \end{align*}
    In matrix form, this becomes
    \begin{equation*}
      \begin{bmatrix}
        1 & 2104 \\
        1 & 1416 \\
        1 & 1534 \\
        2 & 852 
      \end{bmatrix}
      \begin{bmatrix}
        -40 & 200 & -150 \\
        0.25 & 0.1 & 0.4
      \end{bmatrix}
      =
      \begin{bmatrix}
        486 & 410 & 692 \\
        314 & 342 & 416 \\
        344 & 353 & 464 \\
        173 & 285 & 191
      \end{bmatrix}
    \end{equation*}
  \item Matrix Multiplication Properties: Not commutative. $AB \neq BA$. But it's associative. $ABC = (AB)C = A(BC)$.
  \item Identity matrix is $I$ such that $AI = IA = A$. $I = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$ in 2D.
  \item Inverse of $A$ is $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.
  \item Transpose of $A$ is $A^{T}$. If $B=A^{T}$, then $B_{ij} = A_{ji}$.
  \item Quiz: 4) $u = \begin{bmatrix}3 \\ -5 \\ 4\end{bmatrix}, v= \begin{bmatrix}1 \\ 2 \\ 5\end{bmatrix}$, then $u^Tv = \begin{bmatrix}3 & -5 & 4\end{bmatrix}\begin{bmatrix}1 \\ 2\\ 5 \end{bmatrix} = -3 + (-10) + 20 = 13$.
\end{itemize}
\end{document}
