\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{array,enumitem,layouts,calc}
\usepackage[left=2.2cm,right=2.2cm,top=2.0cm,bottom=2.0cm]{geometry}
\setlist{nosep,leftmargin=2em,rightmargin=0em}
%\usepackage{mathptmx}
\usepackage[T1]{fontenc}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\newcommand\leftColumn{0.75\textwidth-2\tabcolsep}
\newcommand\rightColumn{0.25\textwidth-2\tabcolsep}
\newcommand\singleColumn{\textwidth-2\tabcolsep}
\newcommand{\ket}[1]{ $\left| #1 \right>$ }
\setlength{\tabcolsep}{0pt}

\begin{document}
\noindent{\large \textbf{30-Aug-2020}: Welcome Video, and What is Machine Learning}
\begin{itemize}
  \item Machine learning: algorithms; supervised, unsupervised, reinforcement, recommender. In this course, also will learn best practices.
\end{itemize}
\hfill \\
{\large \textbf{31-Aug-2020}: Supervised Learning, and Unsupervised Learning}
\begin{itemize}
  \item Supervised learning: right answers are given
  \item Regression: predicts continuous variable output; Classification: predicts discrete values
  \item Classification can have $1,\dots,N,\dots,\infty$ attributes. E.g. benignness/malignancy based on age, or age and tumor size, etc.
  \item Unsupervised learning a.k.a. clustering: Right answers aren't given. For example, news that links to different sources for the same topic.
  \item Cocktail party algorithm: separates two voices in a conversation, with two microphone recordings. Singular value decomposition is key to this algorithm.
  \item When learning machine learning, use Octave
\end{itemize}
\hfill \\
{\large \textbf{1-Sep-2020}: Model Representation, and Cost Function}
\begin{itemize}
  \item Training set notation: $m$ is number of training examples, $x$ are input examples, and $y$ are the output variables. Together, $(x,y)$ form a training example. Also denoted $(x^{(i)},y^{(i)})$.
  \item In a linear regression, $h_{\theta}(x) = \theta_0 + \theta_1 x \equiv h(x)$.
  \item Cost function is 
    \begin{equation*}
      J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
    \end{equation*}
  \item Want to minimize $J$ w.r.t. $\theta_0$ and $\theta_1$.
\end{itemize}
{\large \textbf{4-Sep-2020}: Cost Function, Intuition I\&II; Gradient Descent}
\begin{itemize}
  \item Intuition I; Let $\theta_0=0$, then $\min_{\theta_1}J(\theta_1)$ is what we want
  \item Ex: $h_{\theta}(x)=\theta_1x$ and let $(x,y) = \{ (1,1), (2,2), (3,3) \}$.
    \begin{align*}
      J(\theta_1) &= \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 \\
      &\rightarrow \text{ If $\theta_1=0$, $h_{\theta}(x) \equiv 0$} \\
      J(0) &= \frac{1}{2 \times 3} \left( 1 + 4+9\right) \\ 
      &= \frac{14}{6}
    \end{align*}
  \item $J(\theta_1)$ is parabolic
  \item We want $\min_{\theta}J(\theta)$; here, $\theta_1=1$ satisfies this criterion
  \item Intuition II; Let $\theta_0, \theta_1$ be free in $J(\theta_0, \theta_1)$ and $h_{\theta}(x)$.
  \item $J(\theta_0, \theta_1)$ is a parabloid
  \item Gradient Descent; Use gradient descent to find $(\theta_0, \theta_1)$ that minimizes $J(\theta_0, \theta_1)$.
  \item Differing starting guesses can give different local minima.
  \item Gradient descent algorithm:
    \begin{equation*}
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad \text{for $j=1,2$}
    \end{equation*}
  \item Simultaneously update $\theta_0, \theta_1$, $\alpha$ is called the learning rate.
  \item Ex: $\theta_0=1, \theta_2=2$ and $\theta_j := \theta_j + \sqrt{\theta_0 \theta_1}$.
    \begin{align*}
      \theta_0 &:= \theta_0 + \sqrt{\theta_0 \theta_1} \\
      &= 1 + \sqrt{1 \times 2} \\
      &= 1 + \sqrt{2} \\
      \theta_1 &= \theta_2 + \sqrt{ \theta_0 \theta_1 } \\
      &= 2 + \sqrt{1 \times 2} \quad \text{note here that we used the old value of $\theta_0$} \\
      &= 2 + \sqrt{2}
    \end{align*}
\end{itemize}
{\large \textbf{5-Sep-2020}: Gradient Descent Intuition, Gradient Descent for Linear Regression}
\begin{itemize}
  \item Gradient Descent Intuition: For simplicity, assume $\theta_0=0$
  \item One variable: $\theta_1 := \theta_1 - \alpha \frac{d}{d \theta_1}J(\theta_1)$; Newton-Raphson
  \item If $\alpha$ is too small, convergence may be very slow. If too large, it may miss the minimum.
  \item If $\theta_1$ is already at a local minimum, g.d. leaves $\theta_1$ unchanged since the derivative is zero.
  \item Gradient Descent for Linear Regression: We need derivatives
    \begin{align*}
      \frac{ \partial }{\partial \theta_0} J(\theta_0, \theta_1) &= \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)}\right) \\
      \frac{ \partial }{\partial \theta_1} J(\theta_0, \theta_1) &= \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)}\right) \times x^{(i)}
    \end{align*}
  \item So, gradient descent finds the new $\theta$ variables as
    \begin{align*}
      \theta_0 &:= \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1x^{(i)} - y^{(i)} \right)\\
      \theta_1 &:= \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1x^{(i)} - y^{(i)} \right) \times x^{(i)}
    \end{align*}
  \item This is called ``batch gradient descent''; batch implies looking at all the training examples. This is represented by the $\sum_{i=1}^m$.
  \item Quiz Linear Regression with One Variable: 2) $m= \Delta y/ \Delta x = (1-0.5)/(2-1) = 0.5 \implies y=0.5x + b$; y-intercept is clearly zero since (0,0) is a data point.
  \item 3) $h_{\theta}(x)$; $\theta_0=-1$, $\theta_1=2$; $h_{\theta}(6) = -1 + 2 \times 6 = 11$
\end{itemize}
\hfill \\
{\large \textbf{9-Sep-2020}: Linear Algebra Review}
\begin{itemize}
  \item Matrices and Vectors: Nothing new; in this course, index from 1.
  \item Addition and Scalar Multiplication: Nothing new
  \item Matrix Vector Multiplication: Nothing new;
  \item Ex: Let house sizes be $\{2104, 1416, 1534, 852.\}$. Let the hypothesis be $h_{\theta}(x) = -40 + 0.25x$.
    \begin{equation*}
      \begin{bmatrix}
        1 & 2104 \\
        1 & 1416 \\
        1 & 1534 \\
        2 & 852 
      \end{bmatrix}
      \begin{bmatrix}
        -40 \\ 
        0.25
      \end{bmatrix} 
      =
      \begin{bmatrix}
        -40 \times 1 + 0.25 \times 2104 \\
        -40 \times 1 + 0.25 \times 1416 \\
        -40 \times 1 + 0.25 \times 1534 \\
        -40 \times 1 + 0.25 \times 852
      \end{bmatrix}
      =
      \begin{bmatrix}
        h_{\theta}(2104)\\
        h_{\theta}(1416)\\
        h_{\theta}(1534)\\
        h_{\theta}(852)
      \end{bmatrix}
    \end{equation*}
    This essentially says data matrix $\times$ parameters = prediction
  \item Best to do this with built-in linear algebra function in Octave/Python. You can do it manually in a for-loop, but it'll be really slow.
  \item Matrix Multiplication: Take the same example. Now we have three hypotheses:
    \begin{align*}
      h_{\theta}(x) &= -40 + 0.25x \\
      h_{\theta}(x) &= 200 + 0.1x \\
      h_{\theta}(x) &= -150 + 0.4x 
    \end{align*}
    In matrix form, this becomes
    \begin{equation*}
      \begin{bmatrix}
        1 & 2104 \\
        1 & 1416 \\
        1 & 1534 \\
        2 & 852 
      \end{bmatrix}
      \begin{bmatrix}
        -40 & 200 & -150 \\
        0.25 & 0.1 & 0.4
      \end{bmatrix}
      =
      \begin{bmatrix}
        486 & 410 & 692 \\
        314 & 342 & 416 \\
        344 & 353 & 464 \\
        173 & 285 & 191
      \end{bmatrix}
    \end{equation*}
  \item Matrix Multiplication Properties: Not commutative. $AB \neq BA$. But it's associative. $ABC = (AB)C = A(BC)$.
  \item Identity matrix is $I$ such that $AI = IA = A$. $I = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$ in 2D.
  \item Inverse of $A$ is $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.
  \item Transpose of $A$ is $A^{T}$. If $B=A^{T}$, then $B_{ij} = A_{ji}$.
  \item Quiz: 4) $u = \begin{bmatrix}3 \\ -5 \\ 4\end{bmatrix}, v= \begin{bmatrix}1 \\ 2 \\ 5\end{bmatrix}$, then $u^Tv = \begin{bmatrix}3 & -5 & 4\end{bmatrix}\begin{bmatrix}1 \\ 2\\ 5 \end{bmatrix} = -3 + (-10) + 20 = 13$.
\end{itemize}
\hfill \\
{\large \textbf{10-Sep-2020}: Multiple Features}
\begin{itemize}
  \item Introduce other features: e.g. house price not just a function of square footage; Now, house price vs. sq. footage, age, number of bedrooms, etc.
  \item $n$ is the number of features, $x_j^{(i)}$ represents the value of the $j^{th}$ feature for the $i^{th}$ training example; $x^{(i)}$ is a vector of all the features.
  \item Hypothesis: $h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2 x_2 + \dots + \theta_n x_n$. Let $x_0^{(i)}=1$. Then, we can write this in matrix form as
    \begin{equation*}
      x=\begin{bmatrix}x_0 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}, \quad \theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix} \implies h_{\theta}(x) = \theta^T x
    \end{equation*}
\end{itemize}
\hfill \\
{\large \textbf{11-Sep-2020}: G.D. for Multiple Variables, G.D. in Practice I - Feature Scaling, G.D. in Practice II - Learning Rate, Features and Polynomial Regression, Normal Equation}
\begin{itemize}
  \item Gradient Descent for Multiple Variables: For $n \geq 1$, gradient descent is
    \begin{equation*}
      \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)x_j^{(i)} \quad \text{for $j=0,\dots,n$.}
    \end{equation*}
  \item G.D. in Practice I - Feature Scaling: ensure features have
    similar scales. E.g.: Houses in the data set have 1-5 bedrooms,
    and are between 0-2000 sq. ft. Scale these features to the order
    of 1. So, divide bedrooms by 5 so it's 0-1, and divide square
    footage by 2000, so it's 0-2.
  \item Feature should be $-1 \leq x_i \leq 1$.
  \item Mean renormalization; Subtract off the mean, and then scale. E.g. $x_1= (\text{sq. footage} - 1000)/2000$ and $x_2 = (\text{bedrooms} - 2)/5$. More formally,
    \begin{equation*}
      x_i \rightarrow \frac{x_i - \mu_i}{s_i} \quad \text{(mean renormalization),}
    \end{equation*}
    where $x_i$ is the feature, $\mu_i$ is the mean value of the
    $i^{th}$ feature, and $s_i$ is the range, or standard deviation,
    of the $i^{th}$ feature.
  \item G.D. in Practice II - Learning Rate: We can plot $J(\theta)$ as a function of iterations, $N$; it should be a decreasing function.
  \item If $J(\theta)$ vs $N$ diverges, you need a smaller learning rate, $\alpha$.
  \item If $J(\theta)$ vs $N$ falls, rises, falls, rises, etc., then use a smaller $\alpha$.
  \item Features and Polynomial Regression: In the housing example,
    hypothesis could be $h_{\theta}(x) = \theta_0 + \theta_1 \times
    \text{length} + \theta_2 \times \text{depth}$.  Maybe you think
    the relevant figure is area = length$\times$depth$\equiv x$. The
    hypothesis is $h_{\theta}(x) = \theta_0 + \theta_1 \times x$.
  \item Polynomial regression; e.g. 
    \begin{alignat*}{4}
      &\theta_0 + \theta_1&&x   + \theta_2&&x^2 + \theta_3&&x^3 \\
      &\theta_0 + \theta_1&&x_1 + \theta_2&&x_2 + \theta_3&&x_3 \\
    \end{alignat*}
    where $x_1=x=\text{area}$, $x_2=x^2 = \text{area}^2$, $x_3=x^3=\text{area}^3$. In polynomial regression, feature scaling becomes very important.
  \item Don't just have to have integer powers: e.g. $h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2 x^{1/2}$
  \item Normal Equation: Instead of using gradient descent to find $\min_{\theta}J$, use normal equation to do it analytically.
  \item Intuition; in 1-D, if $J(\theta)=a\theta^2 + b\theta+c$, you can find $dJ/d\theta=0$ to get the extremum. In $N$-D, set $\partial_{\theta_j}J=0$ for $j=1,\dots,N$.
  \item Say you have $m$ training examples, each with $n$ features. Let
    \begin{align*}
      X_{ij} &= x_j^{(i)} \\
      Y_i &= y^{(i)} \\
      \theta &= \left( X^T X \right)^{-1} X^T Y
    \end{align*}
  \item If the training examples are $\left( x^{(1)}, y^{(1)} \right), \dots, \left( x^{(m)}, y^{(m)} \right)$, then
    \begin{equation*}
      x^{(i)} = \begin{bmatrix}x_0^{(i)} \\ \vdots \\ x_n^{(i)}\end{bmatrix}, 
      X = \begin{bmatrix} \left.\vec{x}^{(1)}\right.^T \\ \vdots \\ \left.\vec{x}^{(m)}\right.^T \end{bmatrix},
      Y = \begin{bmatrix} y^{(1)} \\ \vdots \\ y^{(m)} \end{bmatrix},
      \theta = \left( X^T X\right)^{-1} X^T Y,
    \end{equation*}
    where $x_0^{(i)}=1$.
  \item With normal equation method, features don't have to be scaled.
  \item Normal equation method is slow if $n$ is very large; Computing $(X^TX)^{-1}$ is costly. Inverting an $N \times N$ matrix costs $O(N^3)$.
\end{itemize}
\hfill \\
{\large \textbf{12-Sep-2020}: Normal Equation and Non-Invertibility}
\begin{itemize}
  \item What if $X^TX$ is signular? Octave's \texttt{pinv} (pseudo-inverse) takes care of that
  \item Causes; redundancy: e.g. area in ft$^2$ and in m$^2$; too few equations: $m<n$, fewer training examples compared to features, i.e. too few equations, too many unknowns.
  \item Quiz: 1) Midterm exam average $\mu_1 = (89+72+94-69)/4=81$; range is $s_1= 94-69=25$, thus $x_1^{(3)} \rightarrow (x^{(3)}-\mu_1)/s_1 = (94-81)/25=0.52$
  \item 3) $X = \begin{bmatrix}x_0^{(1)} & \dots & x_3^{(1)} \\ \hfill & \vdots & \hfill \\ x_0^{(14)} & \dots & x_3^{(14)} \end{bmatrix}$ is 14$\times$4.
\end{itemize}
\hfill \\ \hfill \\
{\large \textbf{13-Sep-2020}: Octave Quiz}
\begin{itemize}
  \item Quiz: 1) A is 3$\times$2, B is 2$\times$3. Thus, AB and A+B$^T$ are valid
  \item 4) $u,v$ are $7\times1$. Calculate $u \cdot v$. This can be done via $u^Tv$. In Octave, this is \texttt{sum(v.*w)} and \texttt{v'*w}
\end{itemize}
\hfill \\
{\large \textbf{17-Sep-2020}: Classification, Hypothesis Representation, Decision Boundary, Cost Function, Simplified Cost Function and Gradient Descent}
\begin{itemize}
  \item Classification: $y \in \{ 0,1 \}$ (binary), $y \in \{ 0,1,2, \dots, N \}$ (multiclass)
  \item Could fit a linear $h_{\theta}(x) = \theta^Tx$ and classify
    using a threshold of 0.5. Not good, though. Too sensitive to
    outliers. Also, $h_{\theta}(x)$ can be negative.
  \item Hypothesis Representation:
    \begin{align*}
      h_{\theta}(x) &= g(\theta^Tx); \\
      g(z) &= \frac{1}{1+e^{-z}} \quad \text{sigmoid or logistic function; Fermi-Dirac distribution;}\\
      &\quad \implies h_{\theta}(x) = \frac{1}{1 + \exp\left( -\theta^T x\right)}.
    \end{align*}
  \item $h_{\theta}(x)$ represents the probability that $y=1$ on an
    input x. E.g. $\begin{bmatrix}x_0 \\ x_1 \end{bmatrix}
    = \begin{bmatrix}1 \\ \text{tumor size} \end{bmatrix}$. If
    $g(x)=70\%$, then there is a 70\% chance that the tumor is
    malignant.
  \item $h_{\theta}(x) = P \left( \left. y=1 \right| x; \theta \right)$ means ``the probability that $y=1$ given x, parametrized by $\theta$''
  \item Probabilities sum to 1. $P \left( \left. y=1 \right| x; \theta \right) + P \left( \left. y=0 \right| x; \theta \right) = 1$.
  \item Decision Boundary: Can say if $h_{\theta}(x) \geq 0.5 \implies y=1$, $h_{\theta}(x) < 0.5 \implies y=0$.
  \item $g(z) \geq 0.5 \implies z \geq 0$. So, $h_{\theta}(x)=g(\theta^Tx) \geq 0.5 \implies \theta^Tx \geq 0.$ Converse is true for $<0.5$.
  \item Decision Boundary; Say $g(\theta_0 + \theta_1x_1 +
    \theta_2x_2)$. WLOG, let $\theta_1=\theta_2=1$. On an $x_2$-$x_1$
    diagram, this parametrizes a straight line. $x_2 = -x_1 +
    \theta_0$. The decision boundary is the set of points $(x_2,x_1)$
    s.t. $h_{\theta}(x)=0.5$.
  \item Example; $\theta_0=5, \theta_1=-1, \theta_2=0 \implies h_{\theta}(x) = g(5-x_1)$. Decision boundary is implied by $x_1=5$. Where is $h \geq 0.5$? $5-x \geq 0 \implies x_1 \leq 5$. This region corresponds to $y=1$.
  \item Non-Linear Decision Boundaries; $g(\theta_0 + \theta_1x_2 + \theta_2x_2 + \theta_3x_1^2 + \theta_4x_2^2)$.
  \item Cost Function: Training set $\{ (x^{(1)}, y^{(1)}), \dots, (x^{(m)},y^{(m)})\}$, $x^{(i)} = \begin{bmatrix}x_0^{(i)} \\ \vdots \\ x^{(i)}_n\end{bmatrix}$. $m$ training examples, each with $n$ features. $x_0^{(i)} \equiv 1$.
  \item Recall, for linear regression, $J(\theta) = \frac{1}{m} \sum_{i=1}^m \frac{1}{2} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2$. Define
    \begin{equation*}
      \text{cost}(h_{\theta}(x),y) = \frac{1}{2} \left( h_{\theta}(x) - y\right)^2, \quad \text{for linear regression only}
    \end{equation*}
    so $J(\theta) = \frac{1}{m}\sum_{i=1}^m \text{cost}(h_{\theta}(x^{(i)}, y^{(i)})$.
  \item For log. regression, $J(\theta)$ is not convex, i.e. it has many local minima. Need a new cost function.
  \item For log. regression, $\text{cost}(h_{\theta}(x),y) = \begin{cases} -\log(h_{\theta}(x)) & \mbox{if } y=1 \\ -\log(1-h_{\theta}(x)) & \mbox{if } y=0 \end{cases}$
  \item If $y=1$, and $h_{\theta}(x)=1$, the cost=0. As $h_{\theta}(x)
    \rightarrow 0$, $\text{cost} \rightarrow \infty$. Converse is also
    true for $y=0$.
  \item Simplified Cost Function For Gradient Descent: Write out cost function in one equation
    \begin{equation*}
      \text{cost}(h_{\theta}(x),y) = -y \log(h_{\theta}(x)) - (1-y) \log (1 - h_{\theta}(x))
    \end{equation*}
    Plug into $J$.
    \begin{equation*}
      J(\theta) = \frac{1}{m} \sum_{i=1}^m \left( -y^{(i)} \log\left(h_{\theta}(x^{(i)})\right) - (1-y^{(i)}) \log \left(1- h_{\theta}(x) \right) \right)
    \end{equation*}
    Comes from ``max likelihood estimation.'' Then find $\theta$ via $\min_{\theta}J(\theta)$. USe $\theta$ to make predictions $h_{\theta}(x) = 1/(1 + \exp(\theta^Tx))$.
  \item Use g.d.: $\theta_j := \theta_j - \alpha \partial_{\theta_j} J(\theta)$. Computing the derivatives, we have
    \begin{equation*}
      \theta_j := \theta_j - \alpha \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)x_j^{(i)},
    \end{equation*}
    which is the same as for linear regression, though $h$ has a different meaning.
\end{itemize}
\hfill \\
{\large \textbf{2-Oct-2020} Advanced Optimization, Multiclass Classification One-Vs.-All, Problem of Overfitting, Cost Function (Regularization), Reg. Lin. Regr., Reg. Log. Regr.}
\begin{itemize}
  \item Advanced Optimization; nothing new
  \item Multiclass Classification: One-vs.-all; e.g. weather: sunny, rainy, snowy, etc. $h_{\theta}^{(i)}(x) = P \left( \left. y=i \right| x; \theta \right)$ represents the boundary separating class $i$ from the rest.
  \item Quiz; $h_{\theta}(x) = g(\theta_0 + \theta_1x + \theta_2x)$. Let $\theta_0=6$, $\theta_1=-1$, $\theta_2=0$. The argument, $z$, of $g$ is positive.  $z=0 \implies x=6$, and $z \geq 0 \implies x \leq 6$. So, for $y=1$, $x \leq 6$.
  \item Problem of Overfitting; Underfit means high bias, overfit means high variance. Overfitting fails to generalize to new examples. To fix overfitting you can (i) reduce the number of examples, (ii) regularize to reduce the magnitude of the $\theta_i$.
  \item Cost Function (Regularization); Add terms to the cost function such as $1000\theta_3^2$. This will force $\theta_3$ down.
  \item But what if we don't know what features we want to be small? Do
    \begin{equation*}
      J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)^2  + \underbrace{\lambda \sum_{i=1}^m \theta_j^2}_{\text{Regularization term}} \right],
    \end{equation*}
    where we do not penalize $\theta_0$. If $\lambda$ is too large, it underfits.
  \item Regularized Linear Regression; G.D. for lin. regr.:
    \begin{align*}
      \theta_j &:= \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x^{(i)} + \frac{\lambda}{m}\theta_j \right] \\
      &:= \theta_j \left( 1 - \alpha \frac{\lambda}{m} \right) - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x^{(i)},
    \end{align*}
    where $1 - \alpha \lambda/m < 1$ which reduces $\theta_j$.
  \item Normal equation becomes 
    \begin{equation*}
      \theta = \left(x^Tx + \lambda \begin{bmatrix} 0 & & & \\ & 1 & & \\ & & \ddots & \\ & & & 1 \end{bmatrix}\right)^{-1}x^Ty,
    \end{equation*}
    and as long as $\lambda>0$, the matrix will not be singular.
  \item Regularized Log. Regression; Cost function is
    \begin{equation*}
      J(\theta) = - \frac{1}{m} \sum_{i=1}^m \left[y^{(i)} \log h_{\theta}(x^{(i)}) + (1 -y^{(i)})\log \left( 1 - h_{\theta}(x^{(i)}) \right)\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2
    \end{equation*}
  \item G.D. becomes (same cosmetically as for lin. regr.)
    \begin{equation*}
      \theta_j \left( 1 - \alpha \frac{\lambda}{m} \right) - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x^{(i)}.
    \end{equation*}
  \item Use \texttt{fminunc} (unc means unconstrained). For this, you need to give derivatives.
    \begin{align*}
      \frac{ \partial }{\partial \theta_0} J(\theta) &= \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x_0^{(i)} \\
      \frac{ \partial }{\partial \theta_j} J(\theta) &= \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m} \theta_j
    \end{align*}
\end{itemize}
\hfill \\
{\large \textbf{6-Oct-2020} Neural Networks: Non-Linear Hypothesis, Neurons and the Brain}
\begin{itemize}
  \item e.g. identifying a car. Come up with a classification problem. Take images of a car and select two pixels. Take their intensity, and form an ordered pair $(I(p_1),I(p_2))$. Do this for cars and non-cars. Cars and non-cars will lie in different regions.
  \item If images were 50x50 pixels, $n=2500$. (2500 pixels per image). This would make $3\times10^6$ quadratic features. So, log regression with quadratic features doesn't work.
\end{itemize}
\hfill \\
{\large \textbf{7-Oct-2020} Model Representation I and II, Examples and Intutitions I and II}
\begin{itemize}
  \item Model Representation I; A neuron is a logistic unit. Input is $\{x_1,\dots,x_n\} \rightarrow h_{\theta}(x) = g(\theta^Tx)$ where $g(z) = (1+\exp(-z))^{-1}$. Sigmoid activation function.
  \item We call $x_0$ the bias unit, and in neural networks, $\theta$ are called the weights. We call the inputs the ``input layer'', the last layer of neurons the ``output layer,'' and the rest ``hidden layers.''
  \item $a_i^{(j)}$ is the activation of unit $i$ in the layer $j$. $\Theta^{(j)}$ is the matrix of weights controlling the function mapping from layer $j$ to layer $j+1$.
  \item cf. p. 20 of lecture 08.pdf for a detailed picture.
  \item If a network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ is of dimension $s_{j+1} \times (s_j + 1)$.
  \item Model Representation II; Define $z_1^{(2)} = \Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3$ such that $a_1^{(s)} = g(z_1^{(2)})$ \hfill \\
    Then let $z^{(2)} = \begin{bmatrix}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{bmatrix}$, $x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{bmatrix}$. Then, $z^{(2)} = \Theta^{(1)}x$ and $a^{(2)} = g(z^{(2)})$. (These last two are vectors, and $g$ is applied element-wise. Here $\Theta^{(1)}$ is $3 \times 4$)
  \item cf. p. 23 of lecture 08.pdf. We may also call the input layer $a^{(1)}$. To add a bias for $a^{(3)}$, define $a_0^{(2)}=1$. Thus $a^{(2)} \in \mathcal{R}^4$. 
  \item This is forward propagation. In the example, on p. 23, imagine hiding the input layer. Then we have $a_1^{(2)}$, $a_2^{(2)}$, $a_3^{(2)}$ feeding to $a^{(3)}$.
    \begin{align*}
      a^{(3)} &= g( \Theta^{(2)}_{10}x_0 + \Theta^{(2)}_{11}x_1 + \Theta^{(2)}_{12}x_2 + \Theta^{(2)}_{13}x_3 \\
      &= g( \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3) \quad \text{drop some super/subscripts}\\
      &= g( \theta^T x) \rightarrow \text{ just like log. regression.}
    \end{align*}
    The difference comes in that it's not the straight inputs passed to the log regression. It's the ``learned'' features of the first layer.
  \item An architecture refers to how the networks are connected.
  \item Examples and Intutions I; $x_1$ NOR $x_2$ = NOT($x_1$ XOR $x_2$). XOR is True if $x_1$ OR $x_2$ is True.
  \item cf. p. 34 of lecture 08.pdf for a n.n. implementation of AND, OR, and (NOT $x_1$) AND (NOT $x_2$)
  \item For a NOT, use a large negative weight on the neurons other than the bias.
  \item cf. p. 40-42 re: multiclass classification.
\end{itemize}
\hfill \\
{\large \textbf{8-Oct-2020} Cost Function and Back Propagation, Backpropagation Algorithm}
\begin{itemize}
  \item Cost Function and Back Propagation; $L$ = \# of layers, $s_l$ is \# of units in layer $l$, excluding bias.
  \item Binary classification: $s_L=1$, $h_{\theta}(x) \in \mathcal{R}$, $K=1$ (i.e. a real number). $y = \{ 0,1 \}$.\\
    Multiclass classification: $s_L=K$, $h_{\theta}(x) \in \mathcal{R}^K$, $K \geq 3$, $y \in \mathcal{R}^K$ e.g. $\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix}, \begin{bmatrix}0 \\ 1 \\ 0 \\ 0 \end{bmatrix}$, etc.
  \item For a n.n., $h_{\theta}(x) \in \mathcal{R}^K$, $\left[ h_{\theta}(x)\right]_i = i^{th}$ output. The cost function is
    \begin{align*}
      J( \Theta ) &= -\frac{1}{m} \left[ \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log \left( h_{\Theta}(x^{(i)})\right)_k + (1-y_k^{(i)}) \log \left( 1 - \left( h_{\Theta}(x^{(i)}) \right)_k \right) \right] \\
      &+ \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} \left( \Theta_{ji}^{(l)} \right)^2.
    \end{align*}
  \item Backpropagation Algorithm; Wish to get $\min_{\Theta}J(\Theta)$. Need $J(\Theta)$ and $\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta)$ for all $i,j,l$.
  \item Suppose we have just one training example $(x,y)$. See p.6 of lecture 09.pdf for forward propagation.
  \item $\delta^{l}_j$ represents the error of the $j^{th}$ unit in the $l^{th}$ layer. \\
    For the output layer, (cf. p. 7), $\delta_j^{(4)} = a_j^{(4)} - y_j = \left[ h_{\theta}(x)\right]_j - y_j$. Vectorize this to have $\delta^{(4)} = a^{(4)} - y$. 
  \item Look at $\delta^{(3)}$.
    \begin{equation*}
      \delta^{(3)} = \left( \Theta^{(3)} \right)^T \delta^{(4)} \underbrace{.*}_{\text{element-wise}} g^{\prime} \left( z^{(3)} \right), 
    \end{equation*}
    where $g^{\prime}$ is the derivative of the activation function. It can be shown that $g^{\prime}(z^{(3)}) = a^{(3)}.*\left(1-a^{(3)}\right)$. There is no $\delta^{(1)}$ since we assume there is no error to the input.
  \item We call this backpropagation since we start at the output layer and get $\delta^{(4)}$, and use it to get $\delta^{(3)}$, etc.
  \item Without regularization, we have 
    \begin{equation*}
      \frac{ \partial }{\partial \Theta_{ij}^{(l)}} J (\Theta) = a_j^{(l)} \delta_i^{(l+1)}
    \end{equation*}
  \item Now say the training set is $\{ (x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(,)})\}$. Let $\Delta_{ij}^{(l)}=0$ for all $i,j,l$. Pseudocode:
  \item For $i$=1 to $m$:
    \begin{itemize}
      \item  $a^{(1)} = x^{(i)}$
      \item Do forwards prop to get $a^{(l)}$ for $l=2,\dots,L$
      \item Use $y^{(i)}$ to get $\delta^{(L)} = a^{(L)} - y^{(i)}$
      \item Use back propagation to get $\delta^{(L-1)},\dots,\delta^{(2)}$
      \item $Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)} \rightarrow$ vectorize: $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$
    \end{itemize}
    Then, calculate
    \begin{itemize}
      \item $D_{ij}^{(l)} := \frac{1}{m} \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)}$ if $j \neq 0$
      \item $D_{ij}^{(l)} := \frac{1}{m} \Delta_{ij}^{(l)}$ if $j =0$
    \end{itemize}
    \begin{equation*}
      \implies \frac{\partial}{\partial \Theta_{ij}^{(l)}} J( \Theta ) = D_{ij}^{(l)}
    \end{equation*}
      
\end{itemize}
\end{document}
