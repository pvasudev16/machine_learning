\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{array,enumitem,layouts,calc}
\usepackage[left=2.2cm,right=2.2cm,top=2.0cm,bottom=2.0cm]{geometry}
\setlist{nosep,leftmargin=2em,rightmargin=0em}
%\usepackage{mathptmx}
\usepackage[T1]{fontenc}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\newcommand\leftColumn{0.75\textwidth-2\tabcolsep}
\newcommand\rightColumn{0.25\textwidth-2\tabcolsep}
\newcommand\singleColumn{\textwidth-2\tabcolsep}
\newcommand{\ket}[1]{ $\left| #1 \right>$ }
\setlength{\tabcolsep}{0pt}

\begin{document}
\noindent{\large \textbf{30-Aug-2020}: Welcome Video, and What is Machine Learning}
\begin{itemize}
  \item Machine learning: algorithms; supervised, unsupervised, reinforcement, recommender. In this course, also will learn best practices.
\end{itemize}
\hfill \\
{\large \textbf{31-Aug-2020}: Supervised Learning, and Unsupervised Learning}
\begin{itemize}
  \item Supervised learning: right answers are given
  \item Regression: predicts continuous variable output; Classification: predicts discrete values
  \item Classification can have $1,\dots,N,\dots,\infty$ attributes. E.g. benignness/malignancy based on age, or age and tumor size, etc.
  \item Unsupervised learning a.k.a. clustering: Right answers aren't given. For example, news that links to different sources for the same topic.
  \item Cocktail party algorithm: separates two voices in a conversation, with two microphone recordings. Singular value decomposition is key to this algorithm.
  \item When learning machine learning, use Octave
\end{itemize}
\hfill \\
{\large \textbf{1-Sep-2020}: Model Representation, and Cost Function}
\begin{itemize}
  \item Training set notation: $m$ is number of training examples, $x$ are input examples, and $y$ are the output variables. Together, $(x,y)$ form a training example. Also denoted $(x^{(i)},y^{(i)})$.
  \item In a linear regression, $h_{\theta}(x) = \theta_0 + \theta_1 x \equiv h(x)$.
  \item Cost function is 
    \begin{equation*}
      J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
    \end{equation*}
  \item Want to minimize $J$ w.r.t. $\theta_0$ and $\theta_1$.
\end{itemize}
{\large \textbf{4-Sep-2020}: Cost Function, Intuition I\&II; Gradient Descent}
\begin{itemize}
  \item Intuition I; Let $\theta_0=0$, then $\min_{\theta_1}J(\theta_1)$ is what we want
  \item Ex: $h_{\theta}(x)=\theta_1x$ and let $(x,y) = \{ (1,1), (2,2), (3,3) \}$.
    \begin{align*}
      J(\theta_1) &= \frac{1}{2m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 \\
      &\rightarrow \text{ If $\theta_1=0$, $h_{\theta}(x) \equiv 0$} \\
      J(0) &= \frac{1}{2 \times 3} \left( 1 + 4+9\right) \\ 
      &= \frac{14}{6}
    \end{align*}
  \item $J(\theta_1)$ is parabolic
  \item We want $\min_{\theta}J(\theta)$; here, $\theta_1=1$ satisfies this criterion
  \item Intuition II; Let $\theta_0, \theta_1$ be free in $J(\theta_0, \theta_1)$ and $h_{\theta}(x)$.
  \item $J(\theta_0, \theta_1)$ is a parabloid
  \item Gradient Descent; Use gradient descent to find $(\theta_0, \theta_1)$ that minimizes $J(\theta_0, \theta_1)$.
  \item Differing starting guesses can give different local minima.
  \item Gradient descent algorithm:
    \begin{equation*}
      \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad \text{for $j=1,2$}
    \end{equation*}
  \item Simultaneously update $\theta_0, \theta_1$, $\alpha$ is called the learning rate.
  \item Ex: $\theta_0=1, \theta_2=2$ and $\theta_j := \theta_j + \sqrt{\theta_0 \theta_1}$.
    \begin{align*}
      \theta_0 &:= \theta_0 + \sqrt{\theta_0 \theta_1} \\
      &= 1 + \sqrt{1 \times 2} \\
      &= 1 + \sqrt{2} \\
      \theta_1 &= \theta_2 + \sqrt{ \theta_0 \theta_1 } \\
      &= 2 + \sqrt{1 \times 2} \quad \text{note here that we used the old value of $\theta_0$} \\
      &= 2 + \sqrt{2}
    \end{align*}
\end{itemize}
{\large \textbf{5-Sep-2020}: Gradient Descent Intuition, Gradient Descent for Linear Regression}
\begin{itemize}
  \item Gradient Descent Intuition: For simplicity, assume $\theta_0=0$
  \item One variable: $\theta_1 := \theta_1 - \alpha \frac{d}{d \theta_1}J(\theta_1)$; Newton-Raphson
  \item If $\alpha$ is too small, convergence may be very slow. If too large, it may miss the minimum.
  \item If $\theta_1$ is already at a local minimum, g.d. leaves $\theta_1$ unchanged since the derivative is zero.
  \item Gradient Descent for Linear Regression: We need derivatives
    \begin{align*}
      \frac{ \partial }{\partial \theta_0} J(\theta_0, \theta_1) &= \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)}\right) \\
      \frac{ \partial }{\partial \theta_1} J(\theta_0, \theta_1) &= \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)}\right) \times x^{(i)}
    \end{align*}
  \item So, gradient descent finds the new $\theta$ variables as
    \begin{align*}
      \theta_0 &:= \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1x^{(i)} - y^{(i)} \right)\\
      \theta_1 &:= \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1x^{(i)} - y^{(i)} \right) \times x^{(i)}
    \end{align*}
  \item This is called ``batch gradient descent''; batch implies looking at all the training examples. This is represented by the $\sum_{i=1}^m$.
  \item Quiz Linear Regression with One Variable: 2) $m= \Delta y/ \Delta x = (1-0.5)/(2-1) = 0.5 \implies y=0.5x + b$; y-intercept is clearly zero since (0,0) is a data point.
  \item 3) $h_{\theta}(x)$; $\theta_0=-1$, $\theta_1=2$; $h_{\theta}(6) = -1 + 2 \times 6 = 11$
\end{itemize}
\hfill \\
{\large \textbf{9-Sep-2020}: Linear Algebra Review}
\begin{itemize}
  \item Matrices and Vectors: Nothing new; in this course, index from 1.
  \item Addition and Scalar Multiplication: Nothing new
  \item Matrix Vector Multiplication: Nothing new;
  \item Ex: Let house sizes be $\{2104, 1416, 1534, 852.\}$. Let the hypothesis be $h_{\theta}(x) = -40 + 0.25x$.
    \begin{equation*}
      \begin{bmatrix}
        1 & 2104 \\
        1 & 1416 \\
        1 & 1534 \\
        2 & 852 
      \end{bmatrix}
      \begin{bmatrix}
        -40 \\ 
        0.25
      \end{bmatrix} 
      =
      \begin{bmatrix}
        -40 \times 1 + 0.25 \times 2104 \\
        -40 \times 1 + 0.25 \times 1416 \\
        -40 \times 1 + 0.25 \times 1534 \\
        -40 \times 1 + 0.25 \times 852
      \end{bmatrix}
      =
      \begin{bmatrix}
        h_{\theta}(2104)\\
        h_{\theta}(1416)\\
        h_{\theta}(1534)\\
        h_{\theta}(852)
      \end{bmatrix}
    \end{equation*}
    This essentially says data matrix $\times$ parameters = prediction
  \item Best to do this with built-in linear algebra function in Octave/Python. You can do it manually in a for-loop, but it'll be really slow.
  \item Matrix Multiplication: Take the same example. Now we have three hypotheses:
    \begin{align*}
      h_{\theta}(x) &= -40 + 0.25x \\
      h_{\theta}(x) &= 200 + 0.1x \\
      h_{\theta}(x) &= -150 + 0.4x 
    \end{align*}
    In matrix form, this becomes
    \begin{equation*}
      \begin{bmatrix}
        1 & 2104 \\
        1 & 1416 \\
        1 & 1534 \\
        2 & 852 
      \end{bmatrix}
      \begin{bmatrix}
        -40 & 200 & -150 \\
        0.25 & 0.1 & 0.4
      \end{bmatrix}
      =
      \begin{bmatrix}
        486 & 410 & 692 \\
        314 & 342 & 416 \\
        344 & 353 & 464 \\
        173 & 285 & 191
      \end{bmatrix}
    \end{equation*}
  \item Matrix Multiplication Properties: Not commutative. $AB \neq BA$. But it's associative. $ABC = (AB)C = A(BC)$.
  \item Identity matrix is $I$ such that $AI = IA = A$. $I = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$ in 2D.
  \item Inverse of $A$ is $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.
  \item Transpose of $A$ is $A^{T}$. If $B=A^{T}$, then $B_{ij} = A_{ji}$.
  \item Quiz: 4) $u = \begin{bmatrix}3 \\ -5 \\ 4\end{bmatrix}, v= \begin{bmatrix}1 \\ 2 \\ 5\end{bmatrix}$, then $u^Tv = \begin{bmatrix}3 & -5 & 4\end{bmatrix}\begin{bmatrix}1 \\ 2\\ 5 \end{bmatrix} = -3 + (-10) + 20 = 13$.
\end{itemize}
\hfill \\
{\large \textbf{10-Sep-2020}: Multiple Features}
\begin{itemize}
  \item Introduce other features: e.g. house price not just a function of square footage; Now, house price vs. sq. footage, age, number of bedrooms, etc.
  \item $n$ is the number of features, $x_j^{(i)}$ represents the value of the $j^{th}$ feature for the $i^{th}$ training example; $x^{(i)}$ is a vector of all the features.
  \item Hypothesis: $h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2 x_2 + \dots + \theta_n x_n$. Let $x_0^{(i)}=1$. Then, we can write this in matrix form as
    \begin{equation*}
      x=\begin{bmatrix}x_0 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}, \quad \theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix} \implies h_{\theta}(x) = \theta^T x
    \end{equation*}
\end{itemize}
\hfill \\
{\large \textbf{11-Sep-2020}: G.D. for Multiple Variables, G.D. in Practice I - Feature Scaling, G.D. in Practice II - Learning Rate, Features and Polynomial Regression, Normal Equation}
\begin{itemize}
  \item Gradient Descent for Multiple Variables: For $n \geq 1$, gradient descent is
    \begin{equation*}
      \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)x_j^{(i)} \quad \text{for $j=0,\dots,n$.}
    \end{equation*}
  \item G.D. in Practice I - Feature Scaling: ensure features have
    similar scales. E.g.: Houses in the data set have 1-5 bedrooms,
    and are between 0-2000 sq. ft. Scale these features to the order
    of 1. So, divide bedrooms by 5 so it's 0-1, and divide square
    footage by 2000, so it's 0-2.
  \item Feature should be $-1 \leq x_i \leq 1$.
  \item Mean renormalization; Subtract off the mean, and then scale. E.g. $x_1= (\text{sq. footage} - 1000)/2000$ and $x_2 = (\text{bedrooms} - 2)/5$. More formally,
    \begin{equation*}
      x_i \rightarrow \frac{x_i - \mu_i}{s_i} \quad \text{(mean renormalization),}
    \end{equation*}
    where $x_i$ is the feature, $\mu_i$ is the mean value of the
    $i^{th}$ feature, and $s_i$ is the range, or standard deviation,
    of the $i^{th}$ feature.
  \item G.D. in Practice II - Learning Rate: We can plot $J(\theta)$ as a function of iterations, $N$; it should be a decreasing function.
  \item If $J(\theta)$ vs $N$ diverges, you need a smaller learning rate, $\alpha$.
  \item If $J(\theta)$ vs $N$ falls, rises, falls, rises, etc., then use a smaller $\alpha$.
  \item Features and Polynomial Regression: In the housing example,
    hypothesis could be $h_{\theta}(x) = \theta_0 + \theta_1 \times
    \text{length} + \theta_2 \times \text{depth}$.  Maybe you think
    the relevant figure is area = length$\times$depth$\equiv x$. The
    hypothesis is $h_{\theta}(x) = \theta_0 + \theta_1 \times x$.
  \item Polynomial regression; e.g. 
    \begin{alignat*}{4}
      &\theta_0 + \theta_1&&x   + \theta_2&&x^2 + \theta_3&&x^3 \\
      &\theta_0 + \theta_1&&x_1 + \theta_2&&x_2 + \theta_3&&x_3 \\
    \end{alignat*}
    where $x_1=x=\text{area}$, $x_2=x^2 = \text{area}^2$, $x_3=x^3=\text{area}^3$. In polynomial regression, feature scaling becomes very important.
  \item Don't just have to have integer powers: e.g. $h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2 x^{1/2}$
  \item Normal Equation: Instead of using gradient descent to find $\min_{\theta}J$, use normal equation to do it analytically.
  \item Intuition; in 1-D, if $J(\theta)=a\theta^2 + b\theta+c$, you can find $dJ/d\theta=0$ to get the extremum. In $N$-D, set $\partial_{\theta_j}J=0$ for $j=1,\dots,N$.
  \item Say you have $m$ training examples, each with $n$ features. Let
    \begin{align*}
      X_{ij} &= x_j^{(i)} \\
      Y_i &= y^{(i)} \\
      \theta &= \left( X^T X \right)^{-1} X^T Y
    \end{align*}
  \item If the training examples are $\left( x^{(1)}, y^{(1)} \right), \dots, \left( x^{(m)}, y^{(m)} \right)$, then
    \begin{equation*}
      x^{(i)} = \begin{bmatrix}x_0^{(i)} \\ \vdots \\ x_n^{(i)}\end{bmatrix}, 
      X = \begin{bmatrix} \left.\vec{x}^{(1)}\right.^T \\ \vdots \\ \left.\vec{x}^{(m)}\right.^T \end{bmatrix},
      Y = \begin{bmatrix} y^{(1)} \\ \vdots \\ y^{(m)} \end{bmatrix},
      \theta = \left( X^T X\right)^{-1} X^T Y,
    \end{equation*}
    where $x_0^{(i)}=1$.
  \item With normal equation method, features don't have to be scaled.
  \item Normal equation method is slow if $n$ is very large; Computing $(X^TX)^{-1}$ is costly. Inverting an $N \times N$ matrix costs $O(N^3)$.
\end{itemize}
\hfill \\
{\large \textbf{12-Sep-2020}: Normal Equation and Non-Invertibility}
\begin{itemize}
  \item What if $X^TX$ is signular? Octave's \texttt{pinv} (pseudo-inverse) takes care of that
  \item Causes; redundancy: e.g. area in ft$^2$ and in m$^2$; too few equations: $m<n$, fewer training examples compared to features, i.e. too few equations, too many unknowns.
  \item Quiz: 1) Midterm exam average $\mu_1 = (89+72+94-69)/4=81$; range is $s_1= 94-69=25$, thus $x_1^{(3)} \rightarrow (x^{(3)}-\mu_1)/s_1 = (94-81)/25=0.52$
  \item 3) $X = \begin{bmatrix}x_0^{(1)} & \dots & x_3^{(1)} \\ \hfill & \vdots & \hfill \\ x_0^{(14)} & \dots & x_3^{(14)} \end{bmatrix}$ is 14$\times$4.
\end{itemize}
\hfill \\ \hfill \\
{\large \textbf{13-Sep-2020}: Octave Quiz}
\begin{itemize}
  \item Quiz: 1) A is 3$\times$2, B is 2$\times$3. Thus, AB and A+B$^T$ are valid
  \item 4) $u,v$ are $7\times1$. Calculate $u \cdot v$. This can be done via $u^Tv$. In Octave, this is \texttt{sum(v.*w)} and \texttt{v'*w}
\end{itemize}
\hfill \\
{\large \textbf{17-Sep-2020}: Classification, Hypothesis Representation, Decision Boundary, Cost Function, Simplified Cost Function and Gradient Descent}
\begin{itemize}
  \item Classification: $y \in \{ 0,1 \}$ (binary), $y \in \{ 0,1,2, \dots, N \}$ (multiclass)
  \item Could fit a linear $h_{\theta}(x) = \theta^Tx$ and classify
    using a threshold of 0.5. Not good, though. Too sensitive to
    outliers. Also, $h_{\theta}(x)$ can be negative.
  \item Hypothesis Representation:
    \begin{align*}
      h_{\theta}(x) &= g(\theta^Tx); \\
      g(z) &= \frac{1}{1+e^{-z}} \quad \text{sigmoid or logistic function; Fermi-Dirac distribution;}\\
      &\quad \implies h_{\theta}(x) = \frac{1}{1 + \exp\left( -\theta^T x\right)}.
    \end{align*}
  \item $h_{\theta}(x)$ represents the probability that $y=1$ on an
    input x. E.g. $\begin{bmatrix}x_0 \\ x_1 \end{bmatrix}
    = \begin{bmatrix}1 \\ \text{tumor size} \end{bmatrix}$. If
    $g(x)=70\%$, then there is a 70\% chance that the tumor is
    malignant.
  \item $h_{\theta}(x) = P \left( \left. y=1 \right| x; \theta \right)$ means ``the probability that $y=1$ given x, parametrized by $\theta$''
  \item Probabilities sum to 1. $P \left( \left. y=1 \right| x; \theta \right) + P \left( \left. y=0 \right| x; \theta \right) = 1$.
  \item Decision Boundary: Can say if $h_{\theta}(x) \geq 0.5 \implies y=1$, $h_{\theta}(x) < 0.5 \implies y=0$.
  \item $g(z) \geq 0.5 \implies z \geq 0$. So, $h_{\theta}(x)=g(\theta^Tx) \geq 0.5 \implies \theta^Tx \geq 0.$ Converse is true for $<0.5$.
  \item Decision Boundary; Say $g(\theta_0 + \theta_1x_1 +
    \theta_2x_2)$. WLOG, let $\theta_1=\theta_2=1$. On an $x_2$-$x_1$
    diagram, this parametrizes a straight line. $x_2 = -x_1 +
    \theta_0$. The decision boundary is the set of points $(x_2,x_1)$
    s.t. $h_{\theta}(x)=0.5$.
  \item Example; $\theta_0=5, \theta_1=-1, \theta_2=0 \implies h_{\theta}(x) = g(5-x_1)$. Decision boundary is implied by $x_1=5$. Where is $h \geq 0.5$? $5-x \geq 0 \implies x_1 \leq 5$. This region corresponds to $y=1$.
  \item Non-Linear Decision Boundaries; $g(\theta_0 + \theta_1x_2 + \theta_2x_2 + \theta_3x_1^2 + \theta_4x_2^2)$.
  \item Cost Function: Training set $\{ (x^{(1)}, y^{(1)}), \dots, (x^{(m)},y^{(m)})\}$, $x^{(i)} = \begin{bmatrix}x_0^{(i)} \\ \vdots \\ x^{(i)}_n\end{bmatrix}$. $m$ training examples, each with $n$ features. $x_0^{(i)} \equiv 1$.
  \item Recall, for linear regression, $J(\theta) = \frac{1}{m} \sum_{i=1}^m \frac{1}{2} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2$. Define
    \begin{equation*}
      \text{cost}(h_{\theta}(x),y) = \frac{1}{2} \left( h_{\theta}(x) - y\right)^2, \quad \text{for linear regression only}
    \end{equation*}
    so $J(\theta) = \frac{1}{m}\sum_{i=1}^m \text{cost}(h_{\theta}(x^{(i)}, y^{(i)})$.
  \item For log. regression, $J(\theta)$ is not convex, i.e. it has many local minima. Need a new cost function.
  \item For log. regression, $\text{cost}(h_{\theta}(x),y) = \begin{cases} -\log(h_{\theta}(x)) & \mbox{if } y=1 \\ -\log(1-h_{\theta}(x)) & \mbox{if } y=0 \end{cases}$
  \item If $y=1$, and $h_{\theta}(x)=1$, the cost=0. As $h_{\theta}(x)
    \rightarrow 0$, $\text{cost} \rightarrow \infty$. Converse is also
    true for $y=0$.
  \item Simplified Cost Function For Gradient Descent: Write out cost function in one equation
    \begin{equation*}
      \text{cost}(h_{\theta}(x),y) = -y \log(h_{\theta}(x)) - (1-y) \log (1 - h_{\theta}(x))
    \end{equation*}
    Plug into $J$.
    \begin{equation*}
      J(\theta) = \frac{1}{m} \sum_{i=1}^m \left( -y^{(i)} \log\left(h_{\theta}(x^{(i)})\right) - (1-y^{(i)}) \log \left(1- h_{\theta}(x) \right) \right)
    \end{equation*}
    Comes from ``max likelihood estimation.'' Then find $\theta$ via $\min_{\theta}J(\theta)$. USe $\theta$ to make predictions $h_{\theta}(x) = 1/(1 + \exp(\theta^Tx))$.
  \item Use g.d.: $\theta_j := \theta_j - \alpha \partial_{\theta_j} J(\theta)$. Computing the derivatives, we have
    \begin{equation*}
      \theta_j := \theta_j - \alpha \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)x_j^{(i)},
    \end{equation*}
    which is the same as for linear regression, though $h$ has a different meaning.
\end{itemize}
\hfill \\
{\large \textbf{2-Oct-2020} Advanced Optimization, Multiclass Classification One-Vs.-All, Problem of Overfitting, Cost Function (Regularization), Reg. Lin. Regr., Reg. Log. Regr.}
\begin{itemize}
  \item Advanced Optimization; nothing new
  \item Multiclass Classification: One-vs.-all; e.g. weather: sunny, rainy, snowy, etc. $h_{\theta}^{(i)}(x) = P \left( \left. y=i \right| x; \theta \right)$ represents the boundary separating class $i$ from the rest.
  \item Quiz; $h_{\theta}(x) = g(\theta_0 + \theta_1x + \theta_2x)$. Let $\theta_0=6$, $\theta_1=-1$, $\theta_2=0$. The argument, $z$, of $g$ is positive.  $z=0 \implies x=6$, and $z \geq 0 \implies x \leq 6$. So, for $y=1$, $x \leq 6$.
  \item Problem of Overfitting; Underfit means high bias, overfit means high variance. Overfitting fails to generalize to new examples. To fix overfitting you can (i) reduce the number of examples, (ii) regularize to reduce the magnitude of the $\theta_i$.
  \item Cost Function (Regularization); Add terms to the cost function such as $1000\theta_3^2$. This will force $\theta_3$ down.
  \item But what if we don't know what features we want to be small? Do
    \begin{equation*}
      J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)}\right)^2  + \underbrace{\lambda \sum_{i=1}^m \theta_j^2}_{\text{Regularization term}} \right],
    \end{equation*}
    where we do not penalize $\theta_0$. If $\lambda$ is too large, it underfits.
  \item Regularized Linear Regression; G.D. for lin. regr.:
    \begin{align*}
      \theta_j &:= \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x^{(i)} + \frac{\lambda}{m}\theta_j \right] \\
      &:= \theta_j \left( 1 - \alpha \frac{\lambda}{m} \right) - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x^{(i)},
    \end{align*}
    where $1 - \alpha \lambda/m < 1$ which reduces $\theta_j$.
  \item Normal equation becomes 
    \begin{equation*}
      \theta = \left(x^Tx + \lambda \begin{bmatrix} 0 & & & \\ & 1 & & \\ & & \ddots & \\ & & & 1 \end{bmatrix}\right)^{-1}x^Ty,
    \end{equation*}
    and as long as $\lambda>0$, the matrix will not be singular.
  \item Regularized Log. Regression; Cost function is
    \begin{equation*}
      J(\theta) = - \frac{1}{m} \sum_{i=1}^m \left[y^{(i)} \log h_{\theta}(x^{(i)}) + (1 -y^{(i)})\log \left( 1 - h_{\theta}(x^{(i)}) \right)\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2
    \end{equation*}
  \item G.D. becomes (same cosmetically as for lin. regr.)
    \begin{equation*}
      \theta_j \left( 1 - \alpha \frac{\lambda}{m} \right) - \alpha \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x^{(i)}.
    \end{equation*}
  \item Use \texttt{fminunc} (unc means unconstrained). For this, you need to give derivatives.
    \begin{align*}
      \frac{ \partial }{\partial \theta_0} J(\theta) &= \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x_0^{(i)} \\
      \frac{ \partial }{\partial \theta_j} J(\theta) &= \frac{1}{m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m} \theta_j
    \end{align*}
\end{itemize}
\hfill \\
{\large \textbf{6-Oct-2020} Neural Networks: Non-Linear Hypothesis, Neurons and the Brain}
\begin{itemize}
  \item e.g. identifying a car. Come up with a classification problem. Take images of a car and select two pixels. Take their intensity, and form an ordered pair $(I(p_1),I(p_2))$. Do this for cars and non-cars. Cars and non-cars will lie in different regions.
  \item If images were 50x50 pixels, $n=2500$. (2500 pixels per image). This would make $3\times10^6$ quadratic features. So, log regression with quadratic features doesn't work.
\end{itemize}
\hfill \\
{\large \textbf{7-Oct-2020} Model Representation I and II, Examples and Intutitions I and II}
\begin{itemize}
  \item Model Representation I; A neuron is a logistic unit. Input is $\{x_1,\dots,x_n\} \rightarrow h_{\theta}(x) = g(\theta^Tx)$ where $g(z) = (1+\exp(-z))^{-1}$. Sigmoid activation function.
  \item We call $x_0$ the bias unit, and in neural networks, $\theta$ are called the weights. We call the inputs the ``input layer'', the last layer of neurons the ``output layer,'' and the rest ``hidden layers.''
  \item $a_i^{(j)}$ is the activation of unit $i$ in the layer $j$. $\Theta^{(j)}$ is the matrix of weights controlling the function mapping from layer $j$ to layer $j+1$.
  \item cf. p. 20 of lecture 08.pdf for a detailed picture.
  \item If a network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ is of dimension $s_{j+1} \times (s_j + 1)$.
  \item Model Representation II; Define $z_1^{(2)} = \Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3$ such that $a_1^{(s)} = g(z_1^{(2)})$ \hfill \\
    Then let $z^{(2)} = \begin{bmatrix}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{bmatrix}$, $x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{bmatrix}$. Then, $z^{(2)} = \Theta^{(1)}x$ and $a^{(2)} = g(z^{(2)})$. (These last two are vectors, and $g$ is applied element-wise. Here $\Theta^{(1)}$ is $3 \times 4$)
  \item cf. p. 23 of lecture 08.pdf. We may also call the input layer $a^{(1)}$. To add a bias for $a^{(3)}$, define $a_0^{(2)}=1$. Thus $a^{(2)} \in \mathcal{R}^4$. 
  \item This is forward propagation. In the example, on p. 23, imagine hiding the input layer. Then we have $a_1^{(2)}$, $a_2^{(2)}$, $a_3^{(2)}$ feeding to $a^{(3)}$.
    \begin{align*}
      a^{(3)} &= g( \Theta^{(2)}_{10}x_0 + \Theta^{(2)}_{11}x_1 + \Theta^{(2)}_{12}x_2 + \Theta^{(2)}_{13}x_3 \\
      &= g( \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3) \quad \text{drop some super/subscripts}\\
      &= g( \theta^T x) \rightarrow \text{ just like log. regression.}
    \end{align*}
    The difference comes in that it's not the straight inputs passed to the log regression. It's the ``learned'' features of the first layer.
  \item An architecture refers to how the networks are connected.
  \item Examples and Intutions I; $x_1$ NOR $x_2$ = NOT($x_1$ XOR $x_2$). XOR is True if $x_1$ OR $x_2$ is True.
  \item cf. p. 34 of lecture 08.pdf for a n.n. implementation of AND, OR, and (NOT $x_1$) AND (NOT $x_2$)
  \item For a NOT, use a large negative weight on the neurons other than the bias.
  \item cf. p. 40-42 re: multiclass classification.
\end{itemize}
\hfill \\
{\large \textbf{8-Oct-2020} Cost Function and Back Propagation, Backpropagation Algorithm}
\begin{itemize}
  \item Cost Function and Back Propagation; $L$ = \# of layers, $s_l$ is \# of units in layer $l$, excluding bias.
  \item Binary classification: $s_L=1$, $h_{\theta}(x) \in \mathcal{R}$, $K=1$ (i.e. a real number). $y = \{ 0,1 \}$.\\
    Multiclass classification: $s_L=K$, $h_{\theta}(x) \in \mathcal{R}^K$, $K \geq 3$, $y \in \mathcal{R}^K$ e.g. $\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix}, \begin{bmatrix}0 \\ 1 \\ 0 \\ 0 \end{bmatrix}$, etc.
  \item For a n.n., $h_{\theta}(x) \in \mathcal{R}^K$, $\left[ h_{\theta}(x)\right]_i = i^{th}$ output. The cost function is
    \begin{align*}
      J( \Theta ) &= -\frac{1}{m} \left[ \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log \left( h_{\Theta}(x^{(i)})\right)_k + (1-y_k^{(i)}) \log \left( 1 - \left( h_{\Theta}(x^{(i)}) \right)_k \right) \right] \\
      &+ \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} \left( \Theta_{ji}^{(l)} \right)^2.
    \end{align*}
  \item Backpropagation Algorithm; Wish to get $\min_{\Theta}J(\Theta)$. Need $J(\Theta)$ and $\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta)$ for all $i,j,l$.
  \item Suppose we have just one training example $(x,y)$. See p.6 of lecture 09.pdf for forward propagation.
  \item $\delta^{l}_j$ represents the error of the $j^{th}$ unit in the $l^{th}$ layer. \\
    For the output layer, (cf. p. 7), $\delta_j^{(4)} = a_j^{(4)} - y_j = \left[ h_{\theta}(x)\right]_j - y_j$. Vectorize this to have $\delta^{(4)} = a^{(4)} - y$. 
  \item Look at $\delta^{(3)}$.
    \begin{equation*}
      \delta^{(3)} = \left( \Theta^{(3)} \right)^T \delta^{(4)} \underbrace{.*}_{\text{element-wise}} g^{\prime} \left( z^{(3)} \right), 
    \end{equation*}
    where $g^{\prime}$ is the derivative of the activation function. It can be shown that $g^{\prime}(z^{(3)}) = a^{(3)}.*\left(1-a^{(3)}\right)$. There is no $\delta^{(1)}$ since we assume there is no error to the input.
  \item We call this backpropagation since we start at the output layer and get $\delta^{(4)}$, and use it to get $\delta^{(3)}$, etc.
  \item Without regularization, we have 
    \begin{equation*}
      \frac{ \partial }{\partial \Theta_{ij}^{(l)}} J (\Theta) = a_j^{(l)} \delta_i^{(l+1)}
    \end{equation*}
  \item Now say the training set is $\{ (x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(,)})\}$. Let $\Delta_{ij}^{(l)}=0$ for all $i,j,l$. Pseudocode:
  \item For $i$=1 to $m$:
    \begin{itemize}
      \item  $a^{(1)} = x^{(i)}$
      \item Do forwards prop to get $a^{(l)}$ for $l=2,\dots,L$
      \item Use $y^{(i)}$ to get $\delta^{(L)} = a^{(L)} - y^{(i)}$
      \item Use back propagation to get $\delta^{(L-1)},\dots,\delta^{(2)}$
      \item $\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)} \rightarrow$ vectorize: $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$
    \end{itemize}
    Then, calculate
    \begin{itemize}
      \item $D_{ij}^{(l)} := \frac{1}{m} \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)}$ if $j \neq 0$
      \item $D_{ij}^{(l)} := \frac{1}{m} \Delta_{ij}^{(l)}$ if $j =0$
    \end{itemize}
    \begin{equation*}
      \implies \frac{\partial}{\partial \Theta_{ij}^{(l)}} J( \Theta ) = D_{ij}^{(l)}
    \end{equation*}
\end{itemize}
\hfill \\
{\large \textbf{9-Oct-2020}: Backpropagation Intutition, Implementation Note, Gradient Checking}
\begin{itemize}
  \item Backpropagation Intuition; Quiz: Suppose $\Theta_{11}^{(2)} =
    \Theta_{21}^{(2)} = 0$. After back propagation, what can you say
    about $\delta_1^{(3)}$? We know that $\delta_1^{(3)} =
    \Theta_{11}^{(3)} \delta_1^{(4)}$. So, there is insufficient
    information to tell.
  \item Implementation Note; Unrolling Parameters. In Octave, say \texttt{v} is a matrix. Then \texttt{v(:)} makes it a vector.
  \item Gradient Checking; Use symmetric difference. In 1-D
    \begin{equation*}
      \frac{d}{d \theta} J(\theta) = \frac{ J(\theta + \epsilon) - J(\theta - \epsilon) }{2\epsilon}.
    \end{equation*}
  In N-D,
    \begin{equation*}
      \frac{\partial}{\partial \theta_j} J(\theta) = \frac{J( \theta_1, \dots, \theta_j + \epsilon, \dots, \theta_n) - J( \theta_1, \dots, \theta_j - \epsilon, \dots, \theta_n) }{2 \epsilon}
    \end{equation*}
  \item Use this to compare to derivs we get from back-propagation. Don't use it for training! It will be very slow.
\end{itemize}
\hfill \\
{\large \textbf{13-Oct-2020} Random Initialization, Putting it Together, Autonomous Driving}
\begin{itemize}
  \item Random Initialization; Have to give a guess for $\Theta^{(l)}$; can't just guess all 0s. If the initial $\Theta$ are all zero, it means all units are looking at the same features. Initialize $\Theta_{ij}^{(l)}$ to a random $[ -\epsilon, \epsilon ]$.
  \item Putting it Together; Pick some network architecture. Input
    layer will have a size of $x^{(1)}$. Output layer is the number of
    classes. As a default, use one hidden layer. If $>1$ hidden layer,
    then use the same number of units in each hidden layer.
    \item To train a neural network:
      \begin{enumerate}
        \item Randomly initialize the weights
        \item Do forward propagation to get $h_{\Theta}(x^{(i)})$ for any $x^{(i)}$.
        \item Implement code to compute $J( \Theta )$.
        \item Implement back propagation algorithm to get the partial derivatives. \\
          The backpropagation algorithm will loop over the training examples. Get $a^{(l)}$ and $\delta^{(l)}$ for $l=1, \dots, L$.
        \item Use gradient checking to compare partial derivatives from back propagation. Then discard gradient checking.
        \item Use optimization algorithm to minimize $J( \Theta )$.
      \end{enumerate}
    \item Note that $J( \Theta )$ is non-convex. So, the algorithm can get stuck in a local minimum, but this is rare in practice.
    \item Autonomous Driving; nothing new.
\end{itemize}
\hfill \\
{\large \textbf{15-Oct-2020} Deciding What to Try Next, Evaluating Hypotheses, Model Selection \\ and Train/Validation/Test Sets, Diagnosing Bias vs. Variance, Regularization and Bias/Variance}
\begin{itemize}
  \item Deciding What To Try Next; Suppose, for e.g., you've
    implemented regularized linear regression, and it makes
    predictions with large errors. Try (i) get more training e.g.s,
    (ii) reduce the number of features, (iii) try more features, (iv)
    add polynomial features, (v) increase/decrease $\lambda$.
  \item Simple way to rule out solutions: diagnostics. It can take a while to implement, but can ultimately pay off.
  \item Evaluating a Hypothesis; Split the data into a training/test set. Split the data 70/30 randomly.
  \item For lin. regr., analagous to the training set, the test error is $J_{test}(\theta) = 1/(2m_{test}) \sum_{i=1}^m \left( h_{\theta}(x_{test}^{(i)}) - y_{test}^{(i)} \right)^2$.
  \item Analagous $J_{test}$ for log. regr. For log. regr., also have misclassification error.
    \begin{equation*}
      \text{err}(h_{\theta}(x), y) = \begin{cases} 1 & \mbox{if } h_{\theta}(x) \geq 0.5 \mbox{ and } y=0 \mbox{ OR }  \mbox{ if } h_{\theta}(x) < 0.5 \mbox{ and } y=1 \\ 0 & \mbox{otherwise} \end{cases}
    \end{equation*}
    Could then define the test error to be
    \begin{equation*}
      J_{test}(\theta) = \frac{1}{m_{test}} \sum_{i=1}^{m_{test}} \text{err}\left( h_{\theta}(x_{test}^{(i)}) - y_{test}^{(i)} \right) \quad \text{misclassification error}
    \end{equation*}
  \item Model Selection and Train/Validation/Test Sets; How to choose the polynomial degree $d$? \\
    \begin{center}
      \begin{tabular}{ l c c l l }
        $d=1 \quad$ & $h_{\theta}(x) = \theta_0 + \theta_1x \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow $ & $\theta^{(1)} \rightarrow$ & $J_{test}(\theta^{(1)})$ \\
        $d=2 \quad$ & $h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2 \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow$ & $\theta^{(2)} \rightarrow$ & $J_{test}(\theta^{(2)})$ \\
        $\vdots$ & & & & \\
        $d=n \quad$ & $h_{\theta}(x) = \sum_{k=0}^n \theta_k x^k \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow$ & $\theta^{(n)} \rightarrow$ & $J_{test}(\theta^{(n)})$
      \end{tabular}
    \end{center}
    A problem can be that we're essentially fitting $d$ to the training set. E.g. we find that $J_{test}(\theta^{(5)})$ is the lowest, then we've fit $d=5$ to the training set.
  \item Split the data into a training, cross-validation (CV), and test set (60-20-20). Get the training, CV, and test errors. Repeat the process.
    \begin{center}
      \begin{tabular}{ l c c l l }
        $d=1 \quad$ & $h_{\theta}(x) = \theta_0 + \theta_1x \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow $ & $\theta^{(1)} \rightarrow$ & $J_{cv}(\theta^{(1)})$ \\
        $d=2 \quad$ & $h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2 \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow$ & $\theta^{(2)} \rightarrow$ & $J_{cv}(\theta^{(2)})$ \\
        $\vdots$ & & & & \\
        $d=n \quad$ & $h_{\theta}(x) = \sum_{k=0}^n \theta_k x^k \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow$ & $\theta^{(n)} \rightarrow$ & $J_{cv}(\theta^{(n)})$
      \end{tabular}
    \end{center}
    Pick a $d$ such that $J_{cv}$ is the smallest, e.g. $\theta^{(4)}$, and then get $J_{test}(\theta^{(4)})$, and assess it. So, use cross-validation to select the model (here, polynomial degree).
  \item Diagnosing Bias (Underfit) vs. Variance (overfit); cf. lecture 10, slide 17 for a plot of $J_{cv}$, $J_{test}$ vs $d$.
  \item Bias (underfit): $J_{train}(\theta)$ is high; $J_{train} \approx J_{cv}$. \\
    Variance (overfit):  $J_{train}(\theta)$ is low; $J_{train} \ll J_{cv}$. \\
  \item Regularization and Bias/Variance; For linear regression with
    regularization, 
    \begin{equation*}
      J(\theta) = \sum_{i=1}^m\left(h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m}\sum_{j=1}^m \theta_j^2. 
    \end{equation*}
    Suppose we're using a fourth-order polynomial, so $h_{\theta}(x) =
    \sum_{k=0}^4 \theta_kx^k$.\\ For very large $\lambda$, $\theta_1,
    \dots, \theta_4 \approx 0$ (heavily penalized) $\rightarrow$ underfit
    $\rightarrow$ high bias. \\ For very small $\lambda$, $\theta_1,
    \dots, \theta_4 \neq 0$ (very little regularization) $rightarrow$
    overfit $\rightarrow$ high variance.\\ We need a goldilocks $\lambda$.
  \item With regularization, change definitions a bit.
    \begin{align*}
      J_{train}(\theta) &= \frac{1}{2m}\sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 \quad \text{(NO regularization terms)} \\
      J_{cv}(\theta) &= \frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}} \left( h_{\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)} \right)^2\\
      J_{test}(\theta) &= \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}} \left( h_{\theta}(x_{test}^{(i)}) - y_{test}^{(i)} \right)^2
    \end{align*}
  \item So, how to select $\lambda$? Get $\theta$ from $\min_{\theta}J(\theta)$ with the regularization.
    \begin{center}
      \begin{tabular}{ l c c l l }
        $1) \quad$ & $\lambda=0 \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow $ & $\theta^{(1)} \rightarrow$ & $J_{cv}(\theta^{(1)})$ \\
        $2) \quad$ & $\lambda=0.02 \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow$ & $\theta^{(2)} \rightarrow$ & $J_{cv}(\theta^{(2)})$ \\
        $\vdots$ & & & & \\
        $n) \quad$ & $\lambda=10 \rightarrow$ & $\min_{\theta}J(\theta) \rightarrow$ & $\theta^{(n)} \rightarrow$ & $J_{cv}(\theta^{(n)})$
      \end{tabular}
    \end{center}
    Pick a model that gives the lowest sum of squares $J_{cv}$, e.g. $\theta^{(5)}$. In other words, we fit $\lambda$ to the c.v. set. Then, see how well $J_{test}(\theta^{(5)})$ is.
  \item See lecture 10, slide 23 for a plot of $J_{train}, J_{cv}$ vs. $\lambda$.
\end{itemize}
\hfill \\
{\large \textbf{16-Oct-2020} Learning Curves, Deciding What To Do Next}
\begin{itemize}
  \item Learning Curves; Plot $J_{cv}$/$J_{train}$ as a function of number of training
    examples, $m$.ARtificially reduce the training set to $m$
    examples, for a fixed polynomial degree $h_{\theta}(x)$,
    e.g. $h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2$.
  \item High bias: See lec. 10, p.26:
    $h_{\theta}(x)=\theta_0+\theta_1x$ doesn't fit the data well. More
    data won't help the straight line fit any better. Adding training
    examples only improes $J_{cv}$ up to a certain point. We find $J_{train}$ approaches $J_{cv}$.\\
    $\implies$ if a learning algorithm has high bias, more training e.g.s won't help!
  \item High variance: see lec.10, p.27: e.g. $h_{\theta}(x) =
    \sum_{k=0}^{100}\theta_k x^k$. For small $m$, it'll fit the data
    very well (overfitting). So $J_{train}$ is small for small
    $m$. But as we include more training examples (larger than the
    polynomial degree), $h_{\theta}(x)$ doesn't generalize well, and
    $J_{train}$ increases. $J_{cv}$ starts out high (with a few
    training examples, $h_{\theta}(x)$ generalizes very poorly. With
    more training e.g.s, $J_{cv}$ decreases. \\
    $\implies$ if a learning algorithm has high variance, more training e.g.s will help!
  \item Deciding What To Do Next Revisited; Recall that we're fitting house prices, but the hypothesis made bad predictions.
    \begin{enumerate}
      \item More training e.g.s $\rightarrow$ fixes high variance
      \item Smaller feature set $\rightarrow$ fixes high variance
      \item More features $\rightarrow$ fixes high bias
      \item Add polynomial features $\rightarrow$ fixes high bias
      \item Decrease $\lambda \rightarrow$ fixes high bias
      \item Increase $\lambda \rightarrow$ fixes high varaince
    \end{enumerate}
  \item Neural Networks: Small networks (few units, few hidden layers)
    are prone to underfitting, but computationally cheaper\\ Large
    networks (more units, more hidden layers) are prone to
    overfitting, and computationally more expensive. \\ You can use
    regularization to address overfitting.
  \item How many hidden layers? A sensible default is one hidden layer. But you can use more layers and check $J_{cv}$ and $J_{test}$.
  \item Quiz: n.n. with one hidden layer and $J_{cv}(\theta) \gg
    J_{train}(\theta)$. Will increasing the number of hidden units
    help? No. Since there is a gap between cross validation and
    training sets, it indicates high variance. So it's overfitting the
    data, and adding more layers/units won't help.
\end{itemize}
\end{document}
